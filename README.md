# Distributed training using `torch.distributed`, `torch.nn.parallel.DistributedDataParallel`
Beginner tutorials: please follow this order:
1. [Intro](https://pytorch.org/tutorials/beginner/ddp_series_intro.html), [Intro2](https://pytorch.org/tutorials/beginner/ddp_series_theory.html)
2. [in depth intro](https://pytorch.org/tutorials/beginner/dist_overview.html)
3. [first code](https://pytorch.org/tutorials/beginner/ddp_series_multigpu.html) please watch this video first [link](https://www.youtube.com/watch?v=-LAtx9Q6DA8&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj&index=3&pp=iAQB)
4. [torch.distributed](https://pytorch.org/tutorials/intermediate/dist_tuto.html), [torch.distributed api reference](https://pytorch.org/docs/stable/distributed.html)
5. [Distributed Data Parallel Architecture](https://pytorch.org/docs/master/notes/ddp.html), [Distributed data Paraller API ref](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html)
6. [hello world code](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints)
7. fllow this tutorial seris from begining to the end [link](https://pytorch.org/tutorials/beginner/ddp_series_intro.html)
